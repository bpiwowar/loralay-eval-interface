{"id": "0806.3537", "output": "Conventional statistical learning theory attempts to bound the number of samples needed to learn to a specified level of accuracy for each of the above models. Specifically, it is enough to bound the VC-dimension of the learning model to determine the number of samples needed. However, if we allow ourselves to change the model, then the VC-dimension of the overall learning algorithm is not finite, and much of statistical learning theory does not directly apply. Accepting that much of the time the complexity of the model cannot be a priori bounded, Structural Risk Minimiza- tion explicitly considers a hierarchy of increasingly complex models. We consider the unrestricted learning model consisting of all computable classifiers. Since the VC-dimension is clearly infinite, there are no uniform bounds (independent of the distribution and the target concept) on the number of samples needed to learn accurately. Yet we still want to guarantee a desired level of accuracy. Rather than deciding on the number of samples a priori, it is natural to allow the learning algorithm to decide when it has seen sufficient labeled samples based on the training samples seen up to now and their labels. We prove that it is impossible to bound the sample complexity of any computable universal learning algorithm, even as a function of the target concept. Depending\n"}
{"id": "0808.2220", "output": "A real in the unit interval is computably enumerable (c.e.) if it is the limit of a computable, increasing sequence of rationals. We identify a real with its infinite binary expansion. A real is (algorithmic) random if its binary expansion is an algorithmic random (infinite) sequence. A prefix-free machine is a Turing machine, shortly, machine, from strings to strings whose domain is a prefix-free set. A machine is universal if it can simulate every machine. Chaitin introduced the halting probability  U of a universal machine U, Chaitin's Omega number is defined and proved that  U is c.e. and random. Calude, Hertling, Khoussainov, Wang and Kuera, Slaman proved that there are no other c.e. random reals. In this paper we prove that every c.e. random real is provably computably enumerable.\n"}
{"id": "0811.1250", "output": "This study proposes ABC-Boost (Adaptive Base Class Boost), a concrete implementation of ABC-Boost. ABC-Boost is based on the following two key ideas: 1. For multi-class classification, popular loss functions for K classes usually assume a constraint such that only the values for K  1 classes are needed. Therefore, we can choose a base class and derive algorithms only for K  1 classes. Thus, we adaptively choose the base class which has the \"worst\" performance. ABC-Boost reduces both the training and testing time by 1/K, which may be quite beneficial when K is small.\n"}
{"id": "0812.0197", "output": "We describe a new methodology for studying persistence of topological features across a family of spaces or point-cloud data sets. This theory of zigzag persistence generalises the successful and widely used theory of persistence and persistent homology. We also introduce the Diamond Principle, a calculational tool analogous in power and effect to the Mayer-Vietoris theorem in classical algebraic topology.\n"}
{"id": "1203.4732", "output": "The relational data base model, introduced by Codd in [7], has been particularly successful since it is a mathematically elegant model well suited to describe almost all \"real world\" situations. Since the query languages associated to such model (the relational algebra and the relational calculus) have a formal and simple definition, an interesting field of research is to study the expressive power of such language. A breakthrough in this field has been a syntactic characterization of the set of relations that can be computed in a give data base. These results, also known as BP-completeness, are based on the principle of data independency from the physical representation: the independency that can be extracted from the data base is completely determined at the logical level of such data base. In this paper we introduce a different syntactic characterization of queries computable in a data base. Initially, we will exploit a notion of undifferentiation among two partitions, where each partition represents a level of automorphisms among two sets of relations. Our framework will be applied to analyze a simple graph-based model of the relational data base.\n"}
{"id": "1207.0783", "output": "Biometric authentication systems allow authenticating individuals by comparing a query provided by the claimant to its biometric reference. Usually, the biometric reference is created during the enroll- ment phase by providing one or several captures. However, most biometric modalities are not permanent and system performance decreases with time. To overcome this drawback, it is possible to re-enroll the user at a fixed time. Enrollment period may also be on a too short timespan to collect enough intraclass variabilities to represent the user as best as possible. The aim of semi-supervised template update systems is to address these issues by automatically updating the biometric reference of individuals while they use the system. The update system only uses information from the query and from the biometric recognition system. Self-update systems allow unimodal system to update the reference automatically after collecting unla- belled data. Co-update systems allow using multimodal systems in order to attract these forgotten samples because one biometric reference is updated based on the classification result of the complementary classifier related to the other biometric reference linked to the other modality. In this paper, we propose a new hybrid template update system. We can see it as both a modification of the way of representing the biometric reference, and the way of updating the user's gallery. A user is\n"}
{"id": "1210.1932", "output": "In this paper, we give a new presentation of multipersistence modules. By using this, we are able to extend a version of the algorithm presented in [4] to all multipersistence modules avoiding the use of the mapping telescope. Our algorithm has polinomial complexity for all multifiltrations and in the one-critical case it essentially coincides with the one in [4].\n"}
{"id": "1302.4020", "output": "The topological interference management problem takes a unified view of both wireless networks and wired networks with linear coding at intermediate nodes, showing that the degrees of freedom (DoF) in the wireless case and the capacity in the wired case, are often the same value in their respective normalized units, determined by the same principles, so much so that a solution to one problem automatically solves the other. The purpose of this work is to explore the problem as we go beyond this limitation. Network topology can vary in a wired network as the linear network coding coefficients are varied, and in a wireless network with frequency-hopping or multi-carrier transmission in frequency selective environments.\n"}
{"id": "1312.4231", "output": "In this paper, we construct a dependence space of matroids and apply it to attribute reduction problems. First, a dependence space of matroids is proposed from the viewpoint of closure operator. Second, we study the dependence space by means of matroids. It is interesting to find that the set of consistent sets and the set of reducts of the dependence space are the family of independent sets and the family of bases of the corresponding matroid, respectively. Therefore, this work provides new viewpoints for studying the issues of attribute reduction.\n"}
{"id": "1512.00344", "output": "Interactions among individuals in a population can be described by networks of who-contacts- whom. Interactions among individuals in a population can be described by networks of who-contacts- whom. Interactions among individuals in a population can be described by networks of who-contacts- whom. Such a preventive rewiring assumes transmission of information which allows people to gather knowledge about the disease status of their neighbours. Therefore, in such networks the contact pattern is no longer static but evolves with the spread of an infectious disease. Pairwise models have been the main approach adopted for the analysis of epidemic dynamics on adaptive networks. This paper aims mainly at comparing the predictions from both modelling methods (pairwise/stochastic) for the initial phase of Susceptible-Infectious-Recovered (SIR) and Susceptible-Exposed-Infectious-Recovered (SEIR) epidemics with preventive rewiring among individuals (so, with an interplay between the spread of the disease and the rewiring process, that is, between disease's dynamics and network dynamics).\n"}
{"id": "1604.06187", "output": "Evolutionary algorithms (EAs) have been successfully used in the areas of music and art. The use of EAs for the generation of art has attracted strong research interest. The focus of study in this paper is how EA processes can be mirrored in image transitions. The focus of study in this paper is how EA processes can be mirrored in image transitions. We use well-known random processes for the evolutionary image transition process. We study the effect of running random walks for short periods of time as part of a mutation operator in a (1+1) EA. Furthermore, we consider the effect of alternating different mutation operators over time. Our results show that the area of evolutionary image transition based on different well studied random processes provides a rich source of artistic possibilities with strong potential for further exploration.\n"}
{"id": "1604.08243", "output": "In this paper, we present a preliminary idea of deploying a lightweight micro cloud infrastructure in the sky using indigenously built low cost, single board computers and lightweight Operating System virtualization technologies such as unikernels/dockers. Our paper lays out the preliminary ideas on such a system that can be effectively deployed on demand.\n"}
{"id": "1605.06560", "output": "Compressing Deep Neural Networks (DNNs) is an important issue in many applications, such as computer vision, speech recognition, natural language processing, and domain adaptation. In this paper, we propose a functional hashing structure for DNN compression, which includes HashedNets as a degenerated special case. Plugged into and jointly trained within the original network, the reconstruction network is of a comparably ignorable size, i.e., at low memory cost. This functional hashing structure includes HashedNets as a degenerated special case, and facilitates less value collisions and better value reconstruction. Shortly denoted as Fun-HashNN, our approach could be further extended with dual space hashing and multi-hops. Since it imposes no restriction on other network design choices (e.g. dropout and weight sparsification), Fun-HashNN can be considered as a standard tool for DNN compression. Experiments on several datasets demonstrate promisingly larger reduction of model sizes and/or less loss on prediction accuracy, compared with HashedNets.\n"}
{"id": "1607.01400", "output": "We propose a clustering-based iterative algorithm to solve certain optimization problems in machine learning when data size is large and thus it becomes impractical to use out-of-the-box algorithms. We rely on the principle of data aggregation and then subsequent disaggregations. While it is standard practice to aggregate the data and then calibrate the machine learning algorithm on aggregated data, we embed this into an iterative framework where initial aggregations are gradually disaggregated to the extent that even an optimal solution is obtainable. We apply our algorithm to three common machine learning problems: least absolute deviation regression (LAD), support vector machines (SVM), and semi-supervised support vector machines (S3 VM). We show that our algorithm outperforms the current state-of-the-art algorithms in the computational experiment.\n"}
{"id": "1609.03234", "output": "Imperfect-information extensive-form games model strategic multi-step scenarios between agents with hidden information, such as auctions, security interactions (both physical and virtual), negotiations, and military situations. Counterfactual Regret Minimization (CFR) is a popular iterative algorithm that minimizes regret independently at each decision point in the game. CFR+, a variant of CFR, was used to essentially solve Limit Texas Hold'em, the largest imperfect-information game ever to be essentially solved. Both computation time and storage space are difficult challenges when solving large imperfect-information games. For example, solving Limit Texas Hold'em required nearly 8 million core hours and a complex, domain-specific streaming compression algorithm to store the 262 TiB of uncompressed data in only 10.9 TiB. This data had to be repeatedly decompressed from disk into memory and then compressed back to disk in order to run CFR+. Regret Based Pruning (RBP) is an improvement to CFR that greatly reduces the computation time needed to solve large games by temporarily pruning suboptimal actions. Specifically, if an action has negative regret, then RBP skips that action for the minimum number of iterations it would take for its regret to become positive in CFR. The skipped iterations are then \"made up\" in a single iteration once pruning\n"}
{"id": "1703.07822", "output": "This paper proposes a data-efficient approach for learning mechanical models of unknown objects and predicting their motion under physical interaction. Instead of learning the object's motion explicitly, a Bayesian optimization technique is used to identify relevant physical parameters, such as mass and friction, through the physics. To predict the motion of the object under a new action, the learned parameters can be used to simulate the action in a physics engine. The results of this simulation can then be used by the robot to predict the effect of its action on the object. To solve the challenge, the same Bayesian optimization technique is used to search the optimal control policy for the robotic hand pushing the object.\n"}
{"id": "1008.2277", "output": "We prove that the regular Gaussian distributions that factorize with respect to a chain graph G with d parameters have positive Lebesgue measure with respect to Rd, whereas those that factorize with respect to G but are not faithful to it have zero Lebesgue measure with respect to Rd. This means that, in the measure-theoretic sense described, almost all the regular Gaussian distributions that factorize with respect to G are faithful to it.\n"}
{"id": "1009.0558", "output": "This paper presents a sliding mode control method for two-level quantum systems to deal with bounded uncertainties in the system Hamiltonian. In particular, we propose two approaches of designing the measurement period for different situations which are dependent on the bound on the uncertainties and the allowed probability of failure. The existing Lyapunov design methods in quantum control rely on perfect knowledge of the initial quantum states and system Hamiltonian. In our approach, once the Lyapunov control steers the quantum system into a sliding mode domain, we make a projective measurement on the system. If the system drifts in the sliding mode domain, the projective measurement can tolerate small uncertainties in the system Hamiltonian.\n"}
{"id": "1206.1948", "output": "In this paper, we introduce the notion of less noisy cognitive interference channel (DM-CIC). We show that there are two different less noisy cognitive channels: the primary-less-noisy and cognitive-less-noisy DM-CIC. In the former, the primary receiver is less noisy than the secondary receiver, whereas it is the opposite in the latter. Afterward, we propose two inner bounds for the DM-CIC; one based on superposition coding, and another one using independent coding. We also prove an outer bound on the capacity of this channel. Finally, we show that for the cognitive-less-noisy DM-CIC the inner and outer bounds coincide, and therefore we establish the capacity region for this class of DM-CIC. This proves that superposition coding is the capacity-achieving scheme in the less noisy DM-CIC, as it is in the less noisy Gaussian cognitive interference channel.\n"}
{"id": "1505.06450", "output": "Barabsi-Albert (BA) model has been very successful in describing many properties of real networks, however, its unlimited growth prediction for degree of nodes is not always in line with reality. As opposed to many realistic situations where aging process causes the hubs to be replaced by newcomers, in BA model an emergent hub will always remain power-ful and prevent newcomers from becoming strong hub. In other word, we need to consider the effect of losing power for old hubs and emergence of new hubs. As a common issue with most of these approaches, the outcome has been requested manually from the model, i.e., it is imposed on the network by adding some new terms to BA equation. The objection against these approaches is that, new terms that they added are not capable to demonstrate the main issue; even more, it is not always possible to find a closed form solution. A good candidate to include history effects could be fractional calculus. Fractional calculus, the generalized form of ordinary differentiation and integration to non- integer order [31–35] has unique features such as nonlocality and memory, making it highly applicable in many fields of science and engineering. In this work, the results from this model carries the system memory.\n"}
{"id": "1606.00541", "output": "In many scientific applications, we need to solve lower triangular problems and upper triangular problems, such as Incomplete LU (ILU) preconditioners, do-main decomposition preconditioners and Gauss-Seidel smoothers for algebraic multigrid solvers. The algorithms for these problems are serial in nature and difficult to parallelize on parallel platforms. In this paper, we introduce our work on speeding triangular solvers. A new matrix format, HEC (Hybrid ELL and CSR), is developed. A new matrix format, ILU(k), ILUT and domain decomposition (Restricted Additive Schwarz) preconditioners are developed. Based on these modified algorithms, ILU(k), ILUT and domain decomposition (Restricted Additive Schwarz) preconditioners are developed. Our parallel triangular solvers can be sped up to seven times faster.\n"}
{"id": "1611.03006", "output": "Genetic Relatedness Test (GRT) is a popular test used to identify whether or not a pair of individuals are closely related, genetically speaking. The standard approach to relative identification is to detect the identity-by-descent (IBD) segments between the individuals, and further identify the degree of relatedness via the amount of shared IBD segments. However, collected genetic data is often impossible to anonymize and hard to protect from intentional or accidental leakage. In this paper, we focus on a popular test offered by many DTC companies, namely, Genetic Relatedness Test (GRT). This is used to identify whether or not a pair of individuals are closely related, genetically speaking. Nevertheless, GRT services available today require individuals to send their genetic data (in plaintext) to possibly untrusted DTC companies. Furthermore, collected genetic data is often impossible to anonymize and hard to protect from intentional or accidental leakage. We focus on genomic data that has already been phased into haplotypes and assume that the haplotypes of a pair of individuals with the same length are both interpreted by letters \"A, G, C, T\", so that the problem of detecting IBD shared segments is reduced to the identification of the shared positions of two equal-length\n"}
{"id": "1105.3427", "output": "This paper deals with the efficient calculation of approximate solutions to a sequence of problems of the form P() where the parameter  is varying slowly. In practice, sequences of problems of the form P() can be solved in the framework of nonlinear model predictive control (MPC). A popular way of solving the optimization problem to calculate the control sequence is using either interior point methods or sequential quadratic programming (SQP). A drawback of using SQP is that this method may require several iterations before convergence and therefore the computation time may be too large for a real-time implementation. A solution to this problem was proposed in [6], where the real-time iteration (RTI) technique was implemented. Extensions to the original idea and some theoretical results are reported in [10,13]. Similar nonlinear MPC algorithms are proposed in [10,13]. In this paper we propose a real-time sequential convex programming (RTSCP) method which combines the RTI technique and the sequential quadratic programming (SCP) algorithm: instead of solving with SCP every P() to full accuracy, RTSCP solves only one convex approximation P cvx (x_k-1; _k) using as a linearization point x_k-1, which is the approximate solution of\n"}
{"id": "1206.6177", "output": "The structural analysis of high-index differential-algebraic equations (DAEs) is an important step in the model building procedure. In this paper, we consider Pryce's method and the symbolic differential elimination package rifsimp. Pryce's method is a robust and reliable method for remedying the drawback of the approach and doing so automatically. This is a powerful way to determine the index of the system, its number of degrees of freedom, and exactly which components should be given initial values. The nice feature of the work is a simple and straightforward method for analysing the structure of a differential algebraic system.\n"}
{"id": "1605.08838", "output": "Dueling bandits have been studied most frequently assuming strong regret, in which the regret is 0 if and only if both pulled arms are optimal. Several algorithms have been devised that assume the existence of a Condorcet winner, i.e., one that is preferred in comparison with each other arm. Zoghi et al. (2015) point out that a Condorcet winner does not necessarily exist, and propose two algorithms, CCB and SCB, which achieve O(N log(T)) strong regret in this setting. However, Zoghi et al. (2015) point out that a Condorcet winner does not necessarily exist, and propose two algorithms, CCB and SCB, which achieve O(N log(T)) strong regret in this setting. We provide an algorithm, Comparing with the Best (CTB), that has expected cumulative utility-based weak regret that is constant in T, and that leverages the dependence between preferences over arms induced by the arm features and utility function to provide excellent empirical performance when prior information is available. While our regret bound's dependence on N is looser than Chen and Frazier (2017) (our dependence is 2 N in the worst case, and is N2d when the utility function is linear over a d-dimensional space\n"}
{"id": "3298748", "output": "L'objectif de ce travail est de développer des méthodes d'analyse et d'interprétation automatique des émotions à partir des mouvements du corps humain.\n"}
{"id": "3298747", "output": "Les fraudes bancaires ont aujourd'hui un impact financier important et nécessitent d'être détectées au plus vite. Ces difficultés s'accompagnent de complexités additionnelles dues au caractère temporel et très déséquilibré des données ainsi qu'à l'évolution constante de patterns ou motifs de fraudes. De plus, dans ce domaine particulier non seules les décisions doivent pouvoir être expliquées, mais le modèle derrière la prise de décision doit être compréhensible. Les modèles d'apprentissage dits \"black boxes\" même expliqués à posteriori, ne peuvent donc être envisagés. C'est pourquoi, l'utilisation d'un langage symbolique, via un apprentissage supervisé de règles de décision est privilégiée.\n"}
{"id": "3282727", "output": "Les systèmes de calculs hautes performances ont vu leur complexité exploser avec la croissance des piles logicielles, les variabilités des processeurs ou l'évolution dynamique de la charge réseau ou mémoire. Cette complexité amène des variations de leur comportement, et notamment en performances. Ainsi, il est de plus en plus difficile de prédire précisément leur évolution. Par exemple, les variations des temps de lecture/écriture de fichiers dans une grille de calcul, et donc de la charge du serveur de fichiers, entraînent des variations des temps d'exécution des tâches soumises par les utilisateurs. Ce type de variations apparaît également dans le contexte de l'intergiciel CiGri. Son but est d'utiliser les ressources libres d'un ensemble de grappes de calculs (ou clusters), en y injectant des tâches de priorité minimale (ou best-effort). Ces dernières proviennent d'applications dites Bag-of-Tasks, constituées d'un grand nombre de tâches indépendantes, donc propices à leur parallélisation. Un des problèmes est lié à la potentielle surcharge du serveur héberg\n"}
{"id": "3317641", "output": "Les simulations sont souvent utilisées pour étudier des systèmes complexes comme les écosystèmes, le trafic routier, la gestion de crises..... Ils sont constitués d'un grand nombre d'entités dont le comportement ainsi que leurs interactions décrivent la trajectoire du système. Bien que nous soyons en présence de systèmes naturellement distribués du fait de leur nature hétérogène organisée, ce type de problème ne permet de placement statique à cause des aspects dynamiques. Une façon donc de distribuer de telles applications est de détecter les organisations en cours de simulation afin de minimiser en particulier les coûts de communication dans le respect d'un bon équilibrage de charge. Cela revient donc à du clustering dynamique (organisations) sous contraintes (équilibrage). Une fois les clusters détectés les entités sont rapprochées par migration en fonction des contraintes de l'application.\n"}
{"id": "3292923", "output": "Le domaine de l'éducation est stimulé dans ces dernières années par de nouvelles tendances visant à améliorer le processus d'apprentissage, L'analyse de l'apprentis-sage (Learning Analytics ou LA) qui permet de comprendre le parcours d'un apprenant s'est considérablement développée ces dernières années. Consacrée à l'analyse des données de formation, elle vise à exploiter le potentiel des quantités de plus en plus importantes de données décrivant les informations personnelles, les données d'interaction et les données académiques générées par l'utilisation des environnements d'apprentissage en ligne (EAL) [1]. Cependant, la diversité des EAL existants complique la tâche d'analyse des données d'apprentissage, cette situation est encore amplifiée par la nécessité de combiner des données provenant de diverses sources. Notre étude s'inscrit dans ce contexte, elle ré-pond aux questions suivantes : sur quelle base peut-on concevoir un outil d'analyse capable de favoriser l'engagement et de prédire la réussite des apprenants en ligne? comment rendre l'outil d'analyse interopérable?\n"}
{"id": "3292753", "output": "L'évaluation de la fluence en lecture est couramment évaluée en mesurant le nombre de mots correctement lus par minute (NMCLM). Cependant, cette méthode se concentre sur la mesure du décodage et de la vitesse uniquement, au détriment de la prosodie, expressivité et phrasé. Cette mesure restrictive entretient ainsi la confusion entre lecteur rapide et bon lecteur, créant des lecteurs rapides mais qui ne comprennent pas forcément le texte lu. Cette prévalence du NMCLM dans l'évaluation de la fluence est en partie due à la difficulté d'évaluation des dimensions prosodiques, notamment leurs conséquences sur le phrasé et l'expressivité des lectures. En effet, s'il est aisé d'évaluer la justesse de la prononciation des mots, il y a de nombreuses façons licites de lire un texte avec expressivité. La prosodie est pourtant une compétence dont l'importance dans l'acquisition de la lecture est de plus en plus reconnue. Les études, menées essentiellement en anglais et en espagnol avec des enfants de primaire, montrent que le développement\n"}
{"id": "3313637", "output": "Le langage DBL (degree-based language) a été conçu pour décrire les variations harmoniques des accords mineurs et des accords majeurs mélodiques par superposition de notes alternées. Ce langage permet d'imaginer l'évolution d'une gamme en utilisant seulement les accords majeurs et les accords majeurs mélodiques.\n"}
{"id": "3313639", "output": "Nous présentons dans cet article l'algorithme qui a conduit à la synthèse de Synopsis Seriation. Sur un plan conceptuel, le worfklow implique l'intervention d'un agent virtuel qui \"écoute\" l'input de Synopsis, le segmente en des segments temporels, et re-arrange ces segments pour maximiser la similarité auditive entre les segments adjacents. L'originalité de notre approche est que l'agent fonctionne purement dans le domaine audio, sans recourir à un système de notation externe tel que MIDI ou MusicXML. De plus, l'agent ne considère pas que l'input suit une structure traditionnelle de sections répétées, telles que le verse-chorus ou les formes AABA. Enfin, l'agent assigne des segments de l'input à une sortie stereophonique en optimisant un double objectif de cohérence temporelle et de cohérence binaurale.\n"}
{"id": "3313612", "output": "L'une des activités encadrées par le projet « Organisations musicales symbiotiques : une révision de la notion de concert de recherche-création faisant appel à l'informatique musicale » a lieu à la MSH PN. Son objectif est de revisiter la pratique actuelle musicale notamment par le biais de la musique en réseau. L'une des activités encadrées par ce projet consiste à réaliser des séances de live patching avec un groupe Marcello Messina UFPB – Federal University of Paraiba, Brésil. Cette pratique a été choisie, car nous organisons depuis 2018 des séances de patching collaboratif, spécialement en rapport avec le développement du logiciel Kiwi.\n"}
{"id": "3320332", "output": "Dans cet article, nous abordons la tâche de la reconnaissance d'entités nommées (NER) qui vise à identifier des entités du monde réel, telles que les noms de personnes, d'organi- sations et de lieux à partir des textes bruts. Nous proposons un modèle NER robuste basé sur une pile de Transformers qui comprend des encodeurs BERT affinés. Nous étudions l'impact d'un tel modèle, et nous concluons que ce type de modèle est adapté à l'extraction d'entités à partir de documents historiques.\n"}
{"id": "3329514", "output": "Le développement de systèmes de traduction automatique statistique repose sur des quantités de données parallèles en constante augmentation (voir par ex. (Smith et al., 2013). Son cout devient donc très important, en termes de temps de calcul et de moyens de calcul et de stockage. Une conséquence est que la disponibilité d'un système pour l'utilisateur final, qui peut parfois correspondre à un besoin pressant (Lewis, 2010), est soit impossible car trop couteuse, soit au mieux loin d'être immédiate. Pourtant, le développement standard d'un tel système produit d'énormes bases de connaissances de traduction dont seule une infime partie est effectivement utilisée. Des travaux précédents (Callison-Burch et al., 2005; Lopez, 2008) permettent d'améliorer cette situation, en ne construisant que la partie utile des ressources de traduction qui sont nécessaires à la traduction d'un texte particulier. Dans cet article, nous allons montrer qu'il est possible d'obtenir des performances très proches d'un système de référence à l'état de l'art, en effectuant à la demande non seulement la sélection des\n"}
{"id": "3321348", "output": "La compréhension et la caractérisation des dynamiques cérébrales représentent un enjeu de recherche important. En pratique, le suivi en ligne des connectivités cérébrales pourrait permettre le développement d'interfaces cerveau-machines capables de détecter, en situation opératoire, des états attentionnels dégradés. L'étude des mécanismes cérébraux et des heuristiques mises en oeuvre par la biologie de l'évolution pourraient en re- tour inspirer de nouveaux algorithmes d'intelligence artificielle.\n"}
{"id": "3297018", "output": "L'Institut Français de Bioinformatique (IFB, www.france-bioinformatique.fr) propose différents services pour le traitement des données des sciences de la vie. Ces services s'appuient sur une infrastructure distribuée entre les plateformes régionales membres de l'IFB proposant deux types d'environnements de calculs et traitements, suivant un modèle de cluster ou celui de cloud computing.\n"}
{"id": "3339652", "output": "Dans cet article, nous nous intéressons à l'optimisation d'une approche d'Asservissement Visuel Virtuel Direct d'une caméra omnidirectionnelle. Nous considérons l'optimisation de l'épaisseur Gaussienne des images comme des traits directs. Les contributions concernent une adaptation de l'optimisation de la pose aux caméras omnidirectionnelles et un renouvellement des règles d'initialisation et d'optimisation de l'épaisseur de l'épaisseur des PGMs.\n"}
{"id": "3339664", "output": "Dans les environnements de transports, et plus particulièrement dans le ferroviaire avec l'arrivée du futur train autonome, la détection et le suivi de personnes est une brique cruciale pour des objectifs de surveillance et de sécurité. Néanmoins, elles ont des caractéristiques visuelles très différentes, il est donc naturel de développer des algorithmes dédiés à chacun de ces types d'image. Cependant, pour des raisons de puissance de calcul limitée, il peut être plus intéressant de disposer d'un algorithme de détection capable de traiter les deux types d'images, sans modification spécifique. Pour accomplir cet objectif, nous proposons une technique d'augmentation de données qui transforme pendant l'entraînement une partie des images rectilinéaires en images à effet fisheye. Ceci constitue avec les différentes expérimentations la principale contribution de ce papier pour la tâche de segmentation d'instance.\n"}
{"id": "3339687", "output": "L'objectif principal du projet AniMov est d'apporter aux éleveurs un outil d'analyse automatique de leur troupeau grâce à un système de vidéo-surveillance. Les éleveurs veulent connaître le comportement des animaux afin de disposer d'indicateurs précis pour piloter l'alimentation et la reproduction du troupeau. L'observation permanente et directe des animaux par un humain n'est évidemment pas envisageable pour des raisons de coût mais également d'altération du comportement des animaux. La présence humaine influence le comporte- ment animal et interdit bien souvent d'observer certaines situations spécifiques. C'est dans ce contexte que nous développons un système de vision permettant d'analyser automatiquement le comportement animal pour suivre les cycles d'activité et les situations anormales. Dans un premier temps, nous effectuons la détection et le suivi des animaux pour ensuite réaliser la reconnaissance des compor- tements. Le travail présenté dans cet article porte principa- lement sur la détection et le suivi des animaux, plus préci- sément des chèvres. La détection et le suivi d'objets multiples restent un\n"}
{"id": "3339670", "output": "La segmentation d'images médicales est un processus qui partitionne une image en régions d'intérêt (organes, lésions, etc.). Les troubles des vaisseaux sanguins hépatiques résultent généralement d'un flux sanguin insuffisant venant ou sortant du foie, induit par la cirrhose et d'autres pathogènes hépatiques (carcinome hépatocellulaire - CHC par exemple). Pour aider les experts dans leurs diagnostics et la planification des traitements liés à ces maladies, une méthode précise de segmentation des vaisseaux est néces-saire en pratique clinique, quelles que soient les modalités d'images utilisées comme la TDM (Tomodensitométrie) ou l'IRM (Imagerie par Résonance Magnétique) par exemple. Avec l'arrivée du Deep Learning (DL) ces dernières années, des méthodes de segmentation des vaisseaux ont été développées avec des réseaux DL sur différentes modalités d'imagerie. Dans cet article, nous proposons une étude comparative de plusieurs modèles d'apprentissage en profondeur combinant des filtres\n"}
{"id": "3292785", "output": "Les structures académiques modifient leurs pratiques d'enseignement afin d'ex-ploiter les nouvelles technologies et les avancées dans le domaine des EIAH, et plus généralement pour répondre aux besoins de la société. Une transformation majeure de ces institutions est le remplacement progressif de cursus pré-définis en faveur d'un modulaire. Dans ce paradigme, un étudiant doit choisir une certaine quantité de cours chaque semestre pour construire son propre parcours, lui donnant la liberté d'être en accord avec ses attentes (e.g. objectifs professionnels). Cela introduit de nombreuses probléma-tiques pour tous les acteurs pédagogiques. Du point de vue des étudiants, la personnalisation du cursus nécessite une capacité de planification importante sur plusieurs années ; et une phase de formation en amont qui l'est tout autant afin de s'assurer que les cours choisis correspondent à leurs attentes et leur apportent les compétences nécessaires. Lors d'un échec ou d'un nouvel objectif de carrière, ils doivent également s'assurer que leur objectif de formation reste atteignable, que les cours choisis sont adéquats, et que les compétences qu'ils ont su acquérir le leur\n"}
{"id": "3339624", "output": "La segmentation des vaisseaux sanguins est une tâche difficile car ils sont fins, connectés et tortueux. Des méthodes d'apprentissage profond ont été développées pour traiter ce problème, mais elles nécessitent une large base de données annotées pour chaque nouvelle application d'intérêt, ce qui est très difficile à construire pour les réseaux. Dans ce papier, plutôt que d'apprendre la tâche de segmentation, nous proposons d'apprendre un terme de régularisation reconnecteur. Ainsi, ce terme se généralise mieux que les approches d'apprentissage profond, et peut être facilement appliqué à d'autres types de données.\n"}
{"id": "3290040", "output": "Les jeux d'apprentissage proposent de nombreux avantages par rapport aux outils d'enseignement classique. En effet, de nombreux auteurs considèrent les jeux sérieux comme prometteurs, notamment pour augmenter l'engagement et la motivation des apprenants, d'autres envisagent ces outils pour favoriser un apprentissage plus constructiviste. Pourtant, leur adoption, notamment par les enseignants, reste rare. Pour favoriser cette adoption, nous faisons l'hypothèse qu'une méthode de conception participative telle que le méta-design pourrait convenir. Nos travaux visent à s'interroger sur cette approche dans le contexte de l'utilisation des jeux sérieux par les enseignants en tant que owners of problems. Plus précisément, les travaux présentés ici rapportent l'amorçage d'un projet dans lequel nous nous intéressons à l'enseignement de la pensée informatique, et à l'étude de la mise en place d'une démarche de méta-design avec des jeux sérieux.\n"}
{"id": "3290068", "output": "Le processus d'internalisation (sociale) de nouvelles informations (sci-entifiques) a fait l'objet d'études dans de nombreux domaines par chercheurs in- téressés par la théorie des Représentations Sociales (RS). Par conséquent, lorsque l'on tente de mettre en oeuvre les résultats des LA dans un cadre éducatif, il est important de comprendre à l'avance la nature de ces représentations. Nos questions de recherche sont : 1/ Quelle est la représentation sociale des LA parmi les parties prenantes? 2/ Quelle est sa structure? et 3/ Y a-t-il des différences entre les profils des différentes parties prenantes?\n"}
{"id": "3339685", "output": "Dans cet article, nous présentons trois méthodes de l'état de l'art dans l'extraction des endmembers. Ces méthodes ont été initialement utilisées dans le domaine de la vision artificielle. Elles sont appliquées à un nouveau domaine: la peinture. Nous comparons et analysons leurs performances et résultats.\n"}
{"id": "3298729", "output": "Les habitats intelligents ont vu croître leur intérêt ces dernières années. Nous nous intéressons plus particulièrement aux modélisations de comportements intra et inter utilisateurs ainsi qu'aux systèmes d'apprentissage par renforcement. Ces travaux se placent dans le domaine de l'Ambient Assisted Living et visent à proposer un nouveau système permettant au logement de s'adapter aux besoins de plusieurs utilisateurs. Pour ce faire, nous devons prendre en compte les retours implicites et explicites de ceux-ci afin d'adapter la prise de décision.\n"}
{"id": "3356549", "output": "Dans le cadre d'une enquête criminelle, il est essentiel d'obtenir un maximum d'informations sur les conditions dans lesquelles le crime a été commis. De nombreuses techniques ont vu le jour ces dernières décennies afin d'exploiter au mieux les indices présents sur une scène de crime. L'une de ces techniques, l'Entomologie Médico-Légale, exploite les indices ento-mologiques (i.e. les insectes nécrophages retrouvés sur ou à proximité de la victime) afin de déterminer l'intervalle post-moterm (IPM). Cependant, la diversité des modèles utilisés par les différents experts amène à s'interroger sur la fiabilité des résultats fournis. Il semble donc judicieux d'intégrer une approche de fusion afin de fournir un résultat global cohérent et objectif. Cet article présente une première solution à ce problème développée dans le cadre du Modèle des Croyances Transférables (MCT).\n"}
{"id": "3359983", "output": "Cet article présente une approche de représentation de l'information en Recherche d'Information basée sur les sous-arbres. L'indexation conceptuelle est basée sur l'utilisation d'ontologies.\n"}
